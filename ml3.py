# -*- coding: utf-8 -*-
"""ml3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dWYtd-5VwWhMjoFmVP0FX2ZErQxkB1-t
"""

import pandas as pd
import math

# --------------------------------
# Step 1: Create dataset
# --------------------------------
data = {
    'Outlook': ['Sunny', 'Sunny', 'Overcast', 'Rain', 'Rain', 'Rain', 'Overcast',
                'Sunny', 'Sunny', 'Rain', 'Sunny', 'Overcast', 'Overcast', 'Rain'],
    'Temperature': ['Hot', 'Hot', 'Hot', 'Mild', 'Cool', 'Cool', 'Mild',
                    'Cool', 'Mild', 'Mild', 'Mild', 'Hot', 'Hot', 'Mild'],
    'Humidity': ['High', 'High', 'High', 'High', 'Normal', 'Normal', 'Normal',
                 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'High'],
    'Wind': ['Weak', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong',
             'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Strong'],
    'PlayTennis': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes',
                    'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No']
}

df = pd.DataFrame(data)
print("Training Dataset:\n")
print(df)

# --------------------------------
# Step 2: Entropy calculation
# --------------------------------
def entropy(target_col):
    values = target_col.value_counts()
    total = len(target_col)
    ent = 0
    for count in values:
        p = count / total
        ent -= p * math.log2(p)
    return ent

# --------------------------------
# Step 3: Information Gain
# --------------------------------
def information_gain(data, feature, target):
    total_entropy = entropy(data[target])
    values = data[feature].unique()

    weighted_entropy = 0
    for value in values:
        subset = data[data[feature] == value]
        weighted_entropy += (len(subset) / len(data)) * entropy(subset[target])

    return total_entropy - weighted_entropy

# --------------------------------
# Step 4: ID3 Algorithm
# --------------------------------
def id3(data, features, target):
    # If all target values same → leaf
    if len(data[target].unique()) == 1:
        return data[target].iloc[0]

    # If no features left → majority class
    if len(features) == 0:
        return data[target].mode()[0]

    # Choose best feature
    gains = {feature: information_gain(data, feature, target) for feature in features}
    best_feature = max(gains, key=gains.get)

    tree = {best_feature: {}}

    for value in data[best_feature].unique():
        subset = data[data[best_feature] == value]
        remaining_features = [f for f in features if f != best_feature]
        tree[best_feature][value] = id3(subset, remaining_features, target)

    return tree

# --------------------------------
# Step 5: Build Decision Tree
# --------------------------------
features = list(df.columns[:-1])
target = 'PlayTennis'

decision_tree = id3(df, features, target)
print("\nDecision Tree:\n")
print(decision_tree)

# --------------------------------
# Step 6: Classify New Sample
# --------------------------------
def classify(sample, tree):
    if not isinstance(tree, dict):
        return tree

    feature = next(iter(tree))
    value = sample.get(feature)

    if value in tree[feature]:
        return classify(sample, tree[feature][value])
    else:
        return "Unknown"

# New sample
new_sample = {
    'Outlook': 'Sunny',
    'Temperature': 'Cool',
    'Humidity': 'High',
    'Wind': 'Strong'
}

prediction = classify(new_sample, decision_tree)

print("\nNew Sample:", new_sample)
print("Predicted Class:", prediction)