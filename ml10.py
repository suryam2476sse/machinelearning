# -*- coding: utf-8 -*-
"""ML10.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iUsc5j6BJwubm9vhA0clvC0hhjh7l1AW
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

# --------------------------------------------------
# 1. Create Sample Dataset (mixture of 2 Gaussians)
# --------------------------------------------------
np.random.seed(42)
data1 = np.random.normal(2, 1, 200)
data2 = np.random.normal(8, 1.5, 200)
X = np.concatenate([data1, data2])

# --------------------------------------------------
# 2. Initialize Parameters
# --------------------------------------------------
k = 2                      # number of clusters
n = len(X)

means = np.random.choice(X, k)        # random means
stds = np.random.rand(k) + 1          # random std dev
weights = np.ones(k) / k              # mixing probabilities

iterations = 50

# --------------------------------------------------
# 3. EM Algorithm
# --------------------------------------------------
for it in range(iterations):

    # ---------- E STEP ----------
    # Responsibility matrix (probability each point belongs to each cluster)
    responsibilities = np.zeros((n, k))

    for j in range(k):
        responsibilities[:, j] = weights[j] * norm.pdf(X, means[j], stds[j])

    # Normalize responsibilities
    responsibilities = responsibilities / responsibilities.sum(axis=1, keepdims=True)

    # ---------- M STEP ----------
    Nk = responsibilities.sum(axis=0)

    for j in range(k):
        means[j] = np.sum(responsibilities[:, j] * X) / Nk[j]
        stds[j]  = np.sqrt(np.sum(responsibilities[:, j] * (X - means[j])**2) / Nk[j])
        weights[j] = Nk[j] / n

    if it % 10 == 0:
        print(f"Iteration {it} | Means = {means}")

# --------------------------------------------------
# 4. Final Results
# --------------------------------------------------
print("\nFinal Parameters:")
for j in range(k):
    print(f"\nCluster {j+1}")
    print("Mean =", means[j])
    print("Std Dev =", stds[j])
    print("Weight =", weights[j])

# --------------------------------------------------
# 5. Visualization
# --------------------------------------------------
x_axis = np.linspace(min(X), max(X), 500)

plt.hist(X, bins=30, density=True, alpha=0.5, label="Data")

for j in range(k):
    plt.plot(x_axis, weights[j] * norm.pdf(x_axis, means[j], stds[j]),
             label=f"Gaussian {j+1}")

plt.title("Expectation Maximization - Gaussian Mixture")
plt.legend()
plt.show()